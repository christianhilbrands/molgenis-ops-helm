prometheus:
  alertmanager:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi
  alertmanagerFiles:
    alertmanager.yml:
      global:
        slack_api_url: ''
      receivers:
        - name: default-receiver
          slack_configs:
            - channel: '#alerts'
              send_resolved: true
              text: "{{ range .Alerts }}{{ .CommonAnnotations.summary }}{{ .CommonAnnotation.desciption }}{{ end }}"
      route:
        group_wait: 10s
        group_interval: 5m
        receiver: default-receiver
        repeat_interval: 3h

  kubeStateMetrics:
    enabled: false
    nodeSelector:
      deployPod: "false"
    resources:
      limits:
        cpu: 10m
        memory: 64Mi
      requests:
        cpu: 10m
        memory: 64Mi

  nodeExporter:
    enabled: false
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 100m
        memory: 30Mi

  pushgateway:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi

  server:
    nodeSelector:
      deployPod: "true"
    extraSecretMounts:
      - name: scrape-secrets
        secretName: scrape-secrets
        mountPath: /etc/secrets
        readOnly: true
    resources:
      limits:
        cpu: 1
        memory: 6Gi
      requests:
        cpu: 1
        memory: 6Gi
  configmapReload:
    resources:
      limits:
        cpu: 100m
        memory: 64Mi
      requests:
        cpu: 100m
        memory: 64Mi
  serverFiles:
    alerts:
      groups:
        - name: nodes
          rules:
            - alert: "Instance Down"
              expr: up == 0
              label:
                severity: "ERROR"
              annotations:
                summary: "Instance Down"
                message: "Instance {{$labels.instance}} is down"
            - alert: OutOfMemory
              expr: node_memory_MemFree / node_memory_MemTotal * 100 < 70
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: "Out of memory (instance {{ $labels.instance }})"
                description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: UnusualNetworkThroughputIn
              expr: sum by (instance) (irate(node_network_receive_bytes[2m])) / 1024 / 1024 > 100
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Unusual network throughput in (instance {{ $labels.instance }})"
                description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: UnusualNetworkThroughputOut
              expr: sum by (instance) (irate(node_network_transmit_bytes[2m])) / 1024 / 1024 > 100
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Unusual network throughput out (instance {{ $labels.instance }})"
                description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: UnusualDiskReadRate
              expr: sum by (instance) (irate(node_disk_read_bytes[2m])) / 1024 / 1024 > 50
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Unusual disk read rate (instance {{ $labels.instance }})"
                description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: UnusualDiskWriteRate
              expr: sum by (instance) (irate(node_disk_written_bytes[2m])) / 1024 / 1024 > 50
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Unusual disk write rate (instance {{ $labels.instance }})"
                description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: OutOfDiskSpace
              expr: node_filesystem_free_bytes{mountpoint ="/rootfs"} / node_filesystem_size_bytes{mountpoint ="/rootfs"} * 100 < 10
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Out of disk space (instance {{ $labels.instance }})"
                description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: OutOfInodes
              expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint ="/rootfs"} * 100 < 10
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Out of inodes (instance {{ $labels.instance }})"
                description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: UnusualDiskReadLatency
              expr: rate(node_disk_read_time_ms[1m]) / rate(node_disk_reads_completed[1m]) > 100
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Unusual disk read latency (instance {{ $labels.instance }})"
                description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: UnusualDiskWriteLatency
              expr: rate(node_disk_write_time_ms[1m]) / rate(node_disk_writes_completed[1m]) > 100
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Unusual disk write latency (instance {{ $labels.instance }})"
                description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: HighCpuLoad
              expr: 100 - (avg by(instance) (irate(node_cpu{mode="idle"}[5m])) * 100) > 80
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High CPU load (instance {{ $labels.instance }})"
                description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: ContextSwitching
              expr: rate(node_context_switches[5m]) > 1000
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Context switching (instance {{ $labels.instance }})"
                description: "Context switching is growing on node (> 1000 / s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
            - alert: SwapIsFillingUp
              expr: (1 - (node_memory_SwapFree / node_memory_SwapTotal)) * 100 > 80
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Swap is filling up (instance {{ $labels.instance }})"
                description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      #  - name: pods
      #    rules:
      #      - alert: "Pod OOMKilled"
      #        expr: kube_pod_container_status_terminated_reason{reason='OOMKilled'} == 1
      #        annotations:
      #          severity: "WARN"
      #          message: "Pod {{$labels.container}} in namespace {{$labels.namespace}} got OOMKilled."
      #  - name: services
      #    rules:
      #      - alert: "Many Previews Up"
      #        expr: sum(kube_service_info{namespace=~"preview.*", service=~".*molgenis"}) > 5
      #        annotations:
      #          severity: "DEBUG"
      #          message: "{{$value}} PR previews are currently deployed"
      #  - name: molgenis
      #    rules:
      #      - alert: "Molgenis Down"
      #        expr: up{job="node_exporter_nodes"} == 0
      #        annotations:
      #          severity: "ERROR"
      #          message: "{{$labels.instance}} is down"
    rules: {}
    prometheus.yml:
      rule_files:
        - /etc/config/rules
        - /etc/config/alerts
      scrape_configs:
        #- job_name: jenkins
        #  metrics_path: '/prometheus/'
        #  scheme: https
        #  static_configs:
        #    - targets: ['jenkins.dev.molgenis.org:443']
        #- job_name: 'molgenis-metrics'
        #  metrics_path: /api/metrics/prometheus
        #  scheme: https
        #  static_configs:
        #    - targets:
        #        - master.dev.molgenis.org:443
        #        - latest.test.molgenis.org:443
        #  basic_auth:
        #    username: prometheus
        #    password_file: '/etc/secrets/molgenis-metrics'
        - job_name: 'nodes'
          static_configs:
          - targets: 
            - molgenis117.gcc.rug.nl:9100
            - molgenis121.gcc.rug.nl:9100
            - molgenis119.gcc.rug.nl:9100
secret:
  molgenisMetrics: "xxxx"
